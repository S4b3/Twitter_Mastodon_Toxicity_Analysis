{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from googleapiclient import discovery\n",
    "import os, glob\n",
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./apikey.txt') as api_file: \n",
    "    API_KEY = api_file.read()\n",
    "    \n",
    "API_CLIENT = api_client = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_perspective(text, languages = None):\n",
    "    \n",
    "    #post = json.load(content)\n",
    "    \n",
    "    analyze_request = {\n",
    "        'comment': { 'text': text },\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK' : {}, 'INSULT': {}, 'PROFANITY' : {}, 'THREAT' : {}}\n",
    "    }\n",
    "    \n",
    "    if(languages):\n",
    "        analyze_request['languages'] = languages\n",
    "    try :\n",
    "        response = API_CLIENT.comments().analyze(body=analyze_request).execute()\n",
    "    except (Exception,HttpError) as err:\n",
    "        if (type(err) == HttpError and err.status_code == 429):\n",
    "            time.sleep(61)\n",
    "            return query_perspective(text=text, languages = languages)\n",
    "        else :\n",
    "            print(\"Catched an exception not dealt with.. we will print and then sleep and resume after 61 seconds. \")\n",
    "            print(\"Exception : \")\n",
    "            print(err)\n",
    "            print(\"Sleeping for 61 seconds...\")\n",
    "            time.sleep(61)\n",
    "            print(\"...Restarted...\")\n",
    "            return query_perspective(text=text, languages = languages)\n",
    "    return response['attributeScores']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1.0, 'b': 1.5, 'c': 2.0, 'd': 3.0, 'e': 4.0, 'f': 4.5, 'g': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "A = [{'a':1,'b':2,'c':3,'d':4,'e':5},{'b':1,'c':2,'d':3,'e':4,'f':5},{'c':1,'d':2,'e':3,'f':4,'g':5}]\n",
    "\n",
    "sums = Counter()\n",
    "counters = Counter()\n",
    "for itemset in A:\n",
    "    sums.update(itemset)\n",
    "    counters.update(itemset.keys())\n",
    "\n",
    "ret = {x: float(sums[x])/counters[x] for x in sums.keys()}\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./scores.csv', 'w+') as scores_file :\n",
    "    scores_file.write('twitter_username,mastodon_username,Twitter_TOXICITY,Twitter_SEVERE_TOXICITY,Twitter_IDENTITY_ATTACK,Twitter_INSULT,Twitter_PROFANITY,Twitter_THREAT,Mastodon_TOXICITY,Mastodon_SEVERE_TOXICITY,Mastodon_IDENTITY_ATTACK,Mastodon_INSULT,Mastodon_PROFANITY,Mastodon_THREAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_users_scores(twitter_username, mastodon_username):\n",
    "    print(f\"Processing {twitter_username}\")\n",
    "    # Example mastodon user : https://techhub.social/@nateplusplus\n",
    "    # extract mastodon server name  \n",
    "    mastodon_server = mastodon_username.split('https://')[1].split('/@')[0]\n",
    "    # extract mastodon user name\n",
    "    mastodon_user = mastodon_username.split('/@')[-1].split('/')[0]\n",
    "    # build folder name as we constructed it before\n",
    "    mastodon_folder = mastodon_server+'_'+mastodon_user\n",
    "    \n",
    "    ## TODO: for folder read all tweets and query api\n",
    "    # 1. extract tweets\n",
    "    try:\n",
    "        tweets = glob.glob(f\"./tweets/{twitter_username}/*.json\")\n",
    "        # 2. extract mastodon posts\n",
    "        mastodon_posts = glob.glob(f\"./mastodon_posts/{mastodon_folder}/*.json\")\n",
    "    except:\n",
    "        print(f\"This user is not present in one of the two socials, going next.\")\n",
    "        return\n",
    "    tweets_amount = len(list(tweets))\n",
    "    mastodon_posts_amount = len(list(mastodon_folder))\n",
    "    \n",
    "    twitter_scores = []\n",
    "    mastodon_scores = []\n",
    "    \n",
    "    languages = set()\n",
    "    print(\"scoring Mastodon\")\n",
    "    for post in tqdm(mastodon_posts) :\n",
    "        with open(post, 'r+') as post_file:\n",
    "            post_json = json.load(post_file)\n",
    "        text = post_json['content']\n",
    "        languages.add(post_json['language'])\n",
    "        \n",
    "        scores = query_perspective(text=text, languages = list(languages))\n",
    "        \n",
    "\n",
    "        mastodon_scores.append({\n",
    "            'TOXICITY': scores['TOXICITY']['summaryScore']['value'],\n",
    "                'SEVERE_TOXICITY': scores['SEVERE_TOXICITY']['summaryScore']['value'],\n",
    "                'IDENTITY_ATTACK' : scores[ 'IDENTITY_ATTACK' ]['summaryScore']['value'],\n",
    "                'INSULT': scores['INSULT']['summaryScore']['value'],\n",
    "                'PROFANITY' : scores[ 'PROFANITY' ]['summaryScore']['value'],\n",
    "                'THREAT' : scores[ 'THREAT' ]['summaryScore']['value']\n",
    "        })\n",
    "          \n",
    "    print(\"scoring Twitter\")  \n",
    "    for tweet in tqdm(tweets) :\n",
    "        with open(tweet, 'r+') as tweet_file:\n",
    "            tweet_json = json.load(tweet_file)\n",
    "        text = tweet_json['text']\n",
    "\n",
    "        scores = query_perspective(text=text, languages= list(languages))\n",
    "            \n",
    "        twitter_scores.append(\n",
    "            {\n",
    "            'TOXICITY': scores['TOXICITY']['summaryScore']['value'],\n",
    "            'SEVERE_TOXICITY': scores['SEVERE_TOXICITY']['summaryScore']['value'],\n",
    "            'IDENTITY_ATTACK' : scores['IDENTITY_ATTACK']['summaryScore']['value'],\n",
    "            'INSULT': scores['INSULT']['summaryScore']['value'],\n",
    "            'PROFANITY' : scores['PROFANITY']['summaryScore']['value'],\n",
    "            'THREAT' : scores['THREAT']['summaryScore']['value']\n",
    "        })\n",
    "        \n",
    "    \n",
    "    sums = Counter()\n",
    "    counters = Counter()\n",
    "    for itemset in mastodon_scores:\n",
    "        sums.update(itemset)\n",
    "        counters.update(itemset.keys())\n",
    "\n",
    "    mastodon_score = {x: float(sums[x])/counters[x] for x in sums.keys()}\n",
    "\n",
    "    sums = Counter()\n",
    "    counters = Counter()\n",
    "    for itemset in twitter_scores:\n",
    "        sums.update(itemset)\n",
    "        counters.update(itemset.keys())\n",
    "\n",
    "    twitter_score = {x: float(sums[x])/counters[x] for x in sums.keys()}\n",
    "    \n",
    "    with open('./scores.csv', 'a+') as scores_file :\n",
    "        scores_file.write(f\"\\n{twitter_username},{mastodon_folder},{twitter_score['TOXICITY']},{twitter_score['SEVERE_TOXICITY']},{twitter_score['IDENTITY_ATTACK']},{twitter_score['INSULT']},{twitter_score['PROFANITY']},{twitter_score['THREAT']},{mastodon_score['TOXICITY']},{mastodon_score['SEVERE_TOXICITY']},{mastodon_score['IDENTITY_ATTACK']},{mastodon_score['INSULT']},{mastodon_score['PROFANITY']},{mastodon_score['THREAT']}\")\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "Processing piesdeperro\n",
      "scoring Mastodon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:01<00:00, 30.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring Twitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 214/473 [06:37<00:37,  6.96it/s] "
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('./users.csv', delimiter=',', index_col=None)\n",
    "with open('./users_processed.txt', 'r+') as users_processed_file :\n",
    "    users_processed = users_processed_file.read().split(',')\n",
    "    print(users_processed)\n",
    "for idx, row in users.iterrows():\n",
    "    if(row['twitter_username'] not in users_processed):\n",
    "        process_users_scores(row['twitter_username'], row['mastodon_username'])\n",
    "        with open('./users_processed.txt', 'a+') as users_processed_file :\n",
    "            users_processed_file.write(f\"{row['twitter_username']},\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1631ead7cc89a6b09affa2756a21727862e547c06a37fe75902032523b437b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
